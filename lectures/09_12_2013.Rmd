Lecture 02/03/04 -- Sept 05/10/12 2013
===============================================================================

Model the data

```{r, warning = FALSE, message = FALSE}
library(lmSupport)
library(ggplot2)
library(psych)
library(lavaan)
rmsea <- function(...) fitMeasures(..., "rmsea")
chisq <- function(...) fitMeasures(..., "chisq")
# Load the apgar dataset
d <- lm.readDat("../data/data_apgar.dat")
data_subset <- d[c("apgar", "gestat", "smokes", "anninc", "prenat")]
covmatrix <- cov(data_subset) 
covmatrix
# Predict APGAR from weight gain
m1 <- lm(apgar ~ wgtgain, d)
summary(m1)
qplot(data = d, x = wgtgain, y = apgar)
```

Standardize the weight gain and APGAR scores, so they are both expressed in standard deviation units. Observe that standardization did not change the _p_-values.

```{r fig.width=7, fig.height=6}
d <-  transform(d, zwgtgain = scale(wgtgain), zapgar = scale(apgar), zsmokes = scale(smokes))
describe(d)
m2 <- lm(zapgar ~ zwgtgain, d)
summary(m2)
```


Regress APGAR on smoking. Standardized categorical variables are hard to interpret.

```{r}
m3 <- lm(apgar ~ smokes, d)
summary(m3)
qplot(data = d, x = smokes, y = apgar) + geom_smooth(method="lm")
# Standardized variables
m4 <- lm(zapgar ~ zsmokes, d)
summary(m4)
qplot(data = d, x = zsmokes, y = zapgar) + geom_smooth(method="lm")
```

Regress APGAR on weight-gain and smoking. The coefficient of weight-gain is predicted change in APGAR for a unit change in weight-gain, when statistically controlling for smoking. The coefficient for smoking is expected change in APGAR if the mother smokes, over and above the effect of weight-gain. Note that the individual predictors for smoking and weight-gain become more significant. By controlling for one variable, the other variable can explain a larger proportion of the variance in the dependent variable. The standardized effects allow us to coarsely compare the effects to each other.

We allow exogenous variables to be correlated, in an unanalyzed relationship. Exogenous variables do not have a cause specified in the modelling. Weight gain and smoking would be exogenous variables in these variables. Double arrow is an unanalyzed relationship. Two single arrows form a mutual influence relationship.

```{r}
m5 <- lm(apgar ~ wgtgain + smokes, d)
summary(m5)
# Standardized variables
m6 <- lm(zapgar ~ zwgtgain + zsmokes, d)
summary(m6)
```

Covariance is the correlation of the variable times the standard deviation of each variable. Covariance takes into account the unit of measurement of the variables. 
cov = r * SD1 * SD2


Covariance matrix has 10 pieces of information. Correlation matrix has 6 pieces of information.

```{r}
# Observed correlation matrix
d <- subset(d, select=c(apgar, gestat, smokes, wgtgain))
round(cor(subset(d, select=c(apgar, gestat, smokes, wgtgain))), 2)
```



// We randomly assign path coefficients. Then we compute correlations.

Why would variables be correlated?

1. A causes B
2. Mediation: A causes B causes C
3. Unanalyzed effects: C causes A,B; C correlates with D, C causes A, D causes B causes E

```
Assumed causal paths:

S <--a--> W
S  --b--> G
S  --c--> A
W  --d--> G
W  --e--> A
G  --f--> A
```
```
Correlation values based on the causal paths:

cor_sw = a
cor_sg = b + a*d
cor_wa = e + d*f + a*c + a*b*f
cor_sa = c + b*f + a*e + a*d*f
cor_wg = d + a*b
cor_ga = f + a*b*f + a*c*d + b*c + d*e
```

Maximum-likelihood tries to find the best values for a:f to generate the model-implied correlation matrix



```{r, tidy = FALSE}
m1 <- '
  # regressions
  gestat ~ smokes + wgtgain
  apgar ~ smokes + wgtgain + gestat
  # residual correlations
  smokes ~~ wgtgain'
fit <- sem(m1, data = d, likelihood = "wishart")
summary(fit, standardized = TRUE)
inspect(fit, "sampstat")
fitted(fit)
# difference between the two covariance matrices
resid(fit) 
```

The model perfectly replicated the observed covariance matrix.

Minimum function test statistic is a chi-squared test statistic.

* If the p-value is significant, we reject the model.
* If the p-value is not significant, we can accept the model.

"Disturbances" are residuals of the model. They are latent variables. They contain all the other causes that were not specified in the model. Usually only have distrubances for the endogenous variables. Disturbances have the same unit of measurement as their associated variable.

The variances in the inspecti.fit give the variances in of the disturbances for the endogenous variables.


```{r}
# Variances of the disturbances for gestat: 20.003
var(d$gestat) # 29.02
# Explaining 31% of the variance
1 - (20.003 / 29.02)
apgar_variance <- fitted(fit)$cov["apgar", "apgar"]
# Disturbances in APGAR: 
apgar_distrurbances <- 1.912
# R squared
1 - (apgar_distrurbances / apgar_variance)
```

### Standardized solution

Don't need to report variances in the standardized solution.

Need to footnote that you ran the model on non-standardized variables and got the values for the standardized solution for that.

```{r, tidy = FALSE}
m1b <- '
  # regressions
  gestat ~ smokes + wgtgain
  apgar ~ smokes + wgtgain + gestat'
fit <- sem(m1b, data=d, likelihood="wishart")
summary(fit, standardized=T)
```

Output does not include unanalyzed relationships. 

Wishart variance divides by n-1. Normal or biased variance divides by n. i

## Weird model
4 exogenous variables, 0 endogenous variables, allowing exogenous vars to correlate. We are going to estimate the four variances.

```{r, tidy = FALSE}
m1b <- '
  # residual correlations
  smokes ~~ wgtgain
  smokes ~~ gestat
  smokes ~~ apgar
  wgtgain ~~ gestat
  wgtgain ~~ apgar
  gestat ~~ apgar'
fit2 <- sem(m1b, d, likelihood = "wishart")
summary(fit2, standardized = TRUE)
```

The weird model has 1 for all the disturbances in the standardized solution. Chi-squared = 0. 

***

Sept. 26, 2013

### Why is the dumb model dumb?

We can feed sample covariances into lavaan, so we have P=10 independent pieces of information. 

```{r}
cov(d)
lower.tri(cov(d), diag = TRUE)
```

But we estimate Q = 10 coefficients:

* Number of path coefficents: 5
* Unanalyzed relationships: 1
* Vars of exogenous variance: 2
* Vars of disturbances: 2

### P < Q: Under-identified models

can't even come up with path coefficients for these models.

### P = Q: Just-identified (fully saturated) models

by definition, you will fully replicate the sample covariance matrix and have chi-squared = 0. You do get path coefficients, however, and levels of statistical significance. On the other hand, you could have done this analysis from regression coefficients.

### P > Q: Over-identified models

We want this. These are more elegant, assume a reduced number of causal paths.

Model 2

Set smokes-apgar and wgtgain-gestat paths to 0.

```{r, tidy = FALSE}
m2 <- '
  # regressions
  gestat ~ smokes
  apgar ~ wgtgain + gestat
  # residual correlations
  smokes ~~ wgtgain'
fit <- sem(m2, data = d, likelihood = "wishart")
summary(fit, standardized = TRUE)
# Data's cov
inspect(fit, "sampstat")
# Model's cov
fitted(fit)
# difference between the two covariance matrices
resid(fit) 
```

We have to reject this model because the deviance between the two covariance matrices is too big. The p-value tells us whether the deviance is significant.

```{r}
fitMeasures(fit, "rmsea")
```

We are falling short of our the ideal of having 10 participants for each parameter estimated.

## Two fit indicators

RMSEA is satisfactory when under 0.05. Chi-squared is unsatisfactory when under 0.05.

## Independence Model

Four perfectly orthogonal, independent variables. Have to fix the covariances to zero.

```{r, tidy = FALSE}
m3 <- '
  smokes ~~ 0*wgtgain
  smokes ~~ 0*gestat
  smokes ~~ 0*apgar
  wgtgain ~~ 0*gestat
  wgtgain ~~ 0*apgar
  apgar ~~ 0*gestat'
fit <- sem(m3, data = d, likelihood = "wishart")
summary(fit)
# Data's cov
inspect(fit, "sampstat")
# Model's cov
fitted(fit)
# difference between the two covariance matrices
resid(fit) 
rmsea(fit)
```

```
P = 10, Q = 0 + 0 + 4 + 0, df = 6

Model                     Chi-squared
Just-identified model     0
Apgar2                    in-between the extremes
Independence model        79.20

Chi^2 = 79.20
```


```{r, tidy = FALSE}
d <- lm.readDat("../data/data_apgar.dat")
d$anninc <- d$anninc / 10
m4 <- '
  gestat ~ smokes
  apgar ~ prenat + gestat + smokes + anninc
  prenat ~ anninc'
fit <- sem(m4, data = d, likelihood = "wishart")
fit
summary(fit, standardized = TRUE, fit.measures = TRUE)
inspect(fit, "sampstat")
fitted(fit)
resid(fit) 
rmsea(fit)
fitMeasures(fit)
```

```
Chi-squared = 2.87, p = 0.413, satisfactory model
rmsea = 0, bc chi^2 < df
```


```{r, tidy = FALSE}
m5 <- '
  gestat ~ smokes
  apgar ~ prenat + gestat
  prenat ~ anninc'
fit_m5 <- sem(m5, data = d, likelihood = "wishart")
fit_m5
summary(fit_m5, standardized = TRUE)
inspect(fit_m5, "sampstat")
fitted(fit_m5)
resid(fit_m5) 
rmsea(fit_m5)
m5c <- '
  # Regressions
  gestat ~ a*smokes
  apgar ~ d*prenat + b*gestat
  prenat ~ c*anninc
  # Define indirect effects
  ab := a*b
  cd := c*d'
fit_m5c <- sem(m5c, data = d, likelihood = "wishart")
summary(fit_m5c, standardized = TRUE, fit.measures = TRUE, rsquare = TRUE)
```

Indirect effects now show up under Defined Parameters. RSquare shows the percentage of variance explained. 1 - RSquare is given in the fifth column, in the Variances section.

```
p = 15, q = 4 + 1 + 2 + 3, df = 5
```

Chi-squared of second model increased S-A and $-A paths were forced to be 0.

### Chi-squared difference test

```{r}
chisq(fit_m5) - chisq(fit)
anova(fit, fit_m5)
```

The decrease in fit (increased deviance) is not significant. Conclude that m5 is more satistfacory than m4. The parameter estimates make sense. It has a non-significant chi-squared.


***

```{r}
sessionInfo()
```
