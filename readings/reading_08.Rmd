Questions about Rex Kline's book (for Tues., Nov. 5)
===============================================================================

> Read pp. 112-118 (chap. 5) and pp. 230-245 (chap. 9) of Rex Kline's book. Be prepared to answer the questions below. Please also do the data analysis exercise described below. 

```{r, warning = FALSE, message = FALSE}
library(lavaan)
d <- read.csv("../data/kabc_cor2.csv", row.names = 1)
cor_mat <- as.matrix(d)
# Transform correlation matrix into a covariance matrix
sd <- c(3.40, 2.40, 2.90, 2.70, 2.70, 4.20, 2.80, 3.00)
covar_matrix <- cor2cov(cor_mat, sd)
covar_matrix <- round(covar_matrix, digits = 3)
```


### 1. What can researchers do when some of their indicators are formulated in one direction and other indicators in the other direction?

Reverse-code all the indicators in one direction, so that all indicators have the same direction.

```{r}
reverse_code <- function(values, min, max) max + min - values
reverse_code(1:5, min = 1, max = 5)
```


### 2. Explain to a novice the difference between unidimensional and multidimensional measurement in factor models.

In unidimensional measurement: 

* each indicator measures a single factor, and 
* measurement errors are independent.

In multidimensional measurement, one of these constraints is violated: 

* an indicator may measure more than one factor, or 
* its measurement errors may covary with the error of another indicator.

When we allow measurement errors to covary, we are saying that the measurement errors have something in common beyond the factors inside the model. When we don't allow errors to covary, we are saying that the correlation between a pair of indicators to due to their shared factor loadings.


#### Sidebar

Suppose we have two covarying factors, each with three independent indicators. We would like to see:

1. Convergent validity: High standardized factor loadings for each indicator, x > 0.70
2. Discriminant validity: Not-excessively-high estimated correlations between the factors, r < 0.90


### 3. What are the major differences between exploratory factor analysis (EFA) and confirmatory factor analysis (CFA)?

In EFA:

* You don't have to formulate hypotheses about factor-indicator correspondences or how many factors are at play.
* Models are unrestricted factor models, which in general may not be identified. That is, each indicator can load onto every factor, making a unique solution of estimates impossible. (There may be infinite number of solutions.)

CFA on the other hand deals with hypothesis-driven, restricted-factor, largely identified models. In CFA, we also allow factors to covary.


### 4. Assuming each indicator is specified to load on only one factor, how do we interpret unstandardized and standardized factor loadings?

* Unstandardized factor loadings are interpreted like regression coefficients. 
    + Except for the coefficient set to 1 to give the factor its scale. This coefficient does not have a standard error and hence no test of significance.
    
* Standardized factor loadings are interpreted like correlations between the two indicators. 
    + If you square one, you get the proportion of variance in the indicator explained by the factor. 
    + Hopefully, the factor will explain more than 50% of the variance in each indicator.


### 5. Why can we use a chi-square difference test to compare a single-factor model to a multi-factor model?

Fixing the correlation between two factors to equal 1 essentially combines the two factors into a single factor. Therefore, the single-factor form of the model is nested within the two-factor version. We can use the chi-square difference test because the single factor model is a special case of the multi-factor model.


### 6. Kline concludes that the two-factor model in Figure 9.1 is unacceptable. Why?

* Model chi-square is significant. Exact-fit hypothesis is rejected.
* Upper bound of RMSEA exceeds 0.10, so poor-fit hypothesis is not rejected.
* Five of the residual correlations have an absolute value > 0.10.
* [_Positively_] Lower bound of RMSEA is less than 0.05, so close-fit hypothesis is not rejected.
* He also didn't like the low power.


### 7. What pattern of results suggests that you specified too many factors? Too few factors?

* Poor discriminant validity: High factor correlations: Too many factors
* Poor convergent validity: Low correlation (standardized estimates) between a factor and its indicators: Too few factors.


### 8. What are we testing when we fix the covariances between multiple factors to zero and we then compare the chi-square of this model to that of another model in which the factors are allowed to covary?

We are doing the _test for orthogonality_. This lets us know whether all of the factor covariances jointly/overall differ significantly from zero.


Data Analysis Exercise
-------------------------------------------------------------------------------

### 9. Analyze the KABC dataset described on pages 234--239.

> Start out with a one-factor model. What chi-square value do you find when you round the values of the data covariance matrix to 3 decimals (which is what I did in the script above)? Note: this value is similar but not identical to the value reported in the book. If you don't get a similar value, check whether you specified N = 200.

```{r, tidy = FALSE}
m1 <- "
  f =~ hand + number + word + gestalt + triangles + 
       spatial + analogies + photo"
fit1 <- cfa(m1, sample.cov = covar_matrix, sample.nobs = 200, 
            likelihood = "wishart")
summary(fit1, fit.measures = TRUE)
fitMeasures(fit1, "chisq")
```


### 10. Now run the two-factor model described in Figure 9.1. 

> Do you find similar fit indices than the ones reported in Table 9.4? What is your value for chi-square and RMSEA? Do you have similar parameter estimates than the ones reported in Table 9.2? Compute your value for the standardized estimate of the path from "F2" to "photo series". Explain your computations.

```{r, tidy = FALSE}
m2 <- "
  f1 =~ hand + number + word
  f2 =~ gestalt + triangles + spatial + analogies + photo"
fit2 <- cfa(m2, sample.cov = covar_matrix, sample.nobs = 200, 
            likelihood = "wishart")
summary(fit2, standardized = TRUE, fit.measures = TRUE, rsquare = TRUE)
# Residuals
residuals(fit2)
residuals(fit2, "cor")
residuals(fit2, "standardized")
# Calculate r_photo from variances
inspect(fit2, "sampstat")
r_squared <- 1 - (3.500 / 9.000)
sqrt(r_squared)
```

The standardized estimates and the factor loadings are the same. The variances all differ slightly (still within 1 unit).


### 11.  Now run a different two-factor model. This time, "number" and "word" load on the first factor, all other variables load on the second factor. Look at as many fit indices that you can get. Is this a satisfactory model?

```{r, tidy = FALSE}
m3 <- "
  f1 =~ number + word
  f2 =~ hand + gestalt + triangles + spatial + analogies + photo"
fit3 <- cfa(m3, sample.cov = covar_matrix, sample.nobs = 200, 
            likelihood = "wishart")
summary(fit3, standardized = TRUE, fit.measures = TRUE, rsquare = TRUE)
residuals(fit3, type = "cor") 
residuals(fit3, type = "standardized")
```

* RMSEA looks good.
* SRMR is small.
* Only one standardized estimate is less than .5
* 4 of 8 indicators variances have R^2 > 0.5.
* Only two residual correlations exceed 0.10.
* Model chi-square is not rejected.
* 3-4 of the residual correlations are significant.

It certainly is an improvement over the other model.

***

```{r}
sessionInfo()
```
